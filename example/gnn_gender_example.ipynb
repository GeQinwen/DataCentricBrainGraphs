{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, time, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import aggr, GCNConv, GINConv, SAGEConv, GraphConv, TransformerConv, ChebConv, GATConv, SGConv, GeneralConv, APPNP, MLP\n",
    "from torch.nn import Linear, Conv1d, MaxPool1d, ModuleList\n",
    "\n",
    "# --------- Dataset path ---------\n",
    "dataset_pt = \"REPLACE_WITH_PATH_TO/processed/HCPGender.pt\"  # <-- set me\n",
    "\n",
    "class Args:\n",
    "    dataset = 'HCPGender'\n",
    "    runs = 5\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    seed = 123\n",
    "    model_list = [\"GCNConv\"] \n",
    "    hidden = 32\n",
    "    hidden_mlp = 64\n",
    "    num_layers = 3\n",
    "    epochs = 100\n",
    "    echo_epoch = 10\n",
    "    batch_size = 16\n",
    "    early_stopping = 0     \n",
    "    lr = 5e-4\n",
    "    weight_decay = 5e-4\n",
    "    dropout = 0.5\n",
    "    num_classes = 2      \n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Repro\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------- Dataset loader ---------------------------- #\n",
    "\n",
    "class _LoadedInMemoryDataset(InMemoryDataset):\n",
    "    \"\"\"Load an existing <root>/processed/<name>.pt file from an absolute path.\"\"\"\n",
    "    def __init__(self, pt_path: str):\n",
    "        self._pt_path = pt_path\n",
    "        root = os.path.dirname(os.path.dirname(pt_path))  # the <root>\n",
    "        super().__init__(root)\n",
    "        self.data, self.slices = torch.load(self._pt_path)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [os.path.basename(self._pt_path)]\n",
    "\n",
    "    def process(self):\n",
    "        raise RuntimeError(\"This loader expects an existing processed .pt file.\")\n",
    "\n",
    "\n",
    "class GenderView(torch.utils.data.Dataset):\n",
    "    \"\"\"Wrap a dataset; map y -> int(gender) while preserving x & edge_index.\n",
    "    Exposes num_features/num_node_features from the base dataset for model init.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds: InMemoryDataset):\n",
    "        self.base = base_ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.base[idx]\n",
    "        gender = int(d.y[0].item()) if d.y.dim() > 0 else int(d.y.item())\n",
    "        return Data(x=d.x, edge_index=d.edge_index, y=torch.tensor(gender, dtype=torch.long))\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        return self.base.num_features\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return self.base.num_node_features\n",
    "\n",
    "# Load and wrap\n",
    "assert os.path.exists(dataset_pt), f\"Dataset .pt not found: {dataset_pt}\"\n",
    "base_ds = _LoadedInMemoryDataset(dataset_pt)\n",
    "gender_dataset = GenderView(base_ds)\n",
    "print(f\"Loaded dataset from: {dataset_pt}\")\n",
    "print(f\"#graphs={len(gender_dataset)}, node feature dim={gender_dataset.num_features}\")\n",
    "\n",
    "\n",
    "# ---------------------------- Split & loaders ---------------------------- #\n",
    "\n",
    "labels = [gender_dataset[i].y.item() for i in range(len(gender_dataset))]\n",
    "\n",
    "all_indices = list(range(len(labels)))\n",
    "train_idx, test_idx = train_test_split(all_indices, test_size=0.2, stratify=labels, random_state=123, shuffle=True)\n",
    "train_labels = [labels[i] for i in train_idx]\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.111111, stratify=train_labels, random_state=123, shuffle=True)  # 0.2 -> 0.1 of full\n",
    "\n",
    "train_list = [gender_dataset[i] for i in train_idx]\n",
    "val_list   = [gender_dataset[i] for i in val_idx]\n",
    "test_list  = [gender_dataset[i] for i in test_idx]\n",
    "\n",
    "train_loader = DataLoader(train_list, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_list,   batch_size=args.batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_list,  batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Splits: train={len(train_list)} val={len(val_list)} test={len(test_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------ Model (same design) ------------------------------ #\n",
    "\n",
    "class GNNs(torch.nn.Module):\n",
    "    def __init__(self, args, ref_dataset, hidden_channels, num_layers, GNN, k=0.6):\n",
    "        super().__init__()\n",
    "        # k-N sort aggregation\n",
    "        if k < 1:  # percentile to absolute\n",
    "            num_nodes = sorted([data.num_nodes for data in ref_dataset])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = max(10, k)\n",
    "        self.k = int(k)\n",
    "        self.sort_aggr = aggr.SortAggregation(self.k)\n",
    "\n",
    "        in_channels = ref_dataset.num_features\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        if GNN is GINConv:\n",
    "            self.convs.append(GINConv(MLP([in_channels, hidden_channels])))\n",
    "            for _ in range(num_layers - 1):\n",
    "                self.convs.append(GINConv(MLP([hidden_channels, hidden_channels])))\n",
    "            self.convs.append(GINConv(MLP([hidden_channels, 1])))\n",
    "        elif GNN is ChebConv:\n",
    "            self.convs.append(ChebConv(in_channels, hidden_channels, K=5))\n",
    "            for _ in range(num_layers - 1):\n",
    "                self.convs.append(ChebConv(hidden_channels, hidden_channels, K=5))\n",
    "            self.convs.append(ChebConv(hidden_channels, 1, K=5))\n",
    "        else:\n",
    "            self.convs.append(GNN(in_channels, hidden_channels))\n",
    "            for _ in range(num_layers - 1):\n",
    "                self.convs.append(GNN(hidden_channels, hidden_channels))\n",
    "            self.convs.append(GNN(hidden_channels, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_channels * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0], conv1d_kws[0])\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1], conv1d_kws[1], 1)\n",
    "\n",
    "        dense_dim = int((self.k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "\n",
    "        self.mlp = MLP([dense_dim, 32, args.num_classes], dropout=0.5, batch_norm=False)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.mlp.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [conv(xs[-1], edge_index).tanh()]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        x = self.sort_aggr(x, batch)     # [num_graphs, k, latent]\n",
    "        x = x.unsqueeze(1)                # [num_graphs, 1, k * latent]\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.conv2(x).relu()\n",
    "        x = x.view(x.size(0), -1)         # [num_graphs, dense_dim]\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------ Train / Eval ------------------------------ #\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total += loss.item() * data.num_graphs\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "for model_name in args.model_list:\n",
    "    print(f\"\\n=== Model: {model_name} ===\")\n",
    "    val_hist, test_hist = [], []\n",
    "    seeds = [123,124,125,126,127]  # args.runs seeds\n",
    "\n",
    "    for run_idx in range(args.runs):\n",
    "        seed = seeds[run_idx % len(seeds)]\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Build model\n",
    "        GNNClass = eval(model_name)\n",
    "        model = GNNs(args, gender_dataset, args.hidden, args.num_layers, GNNClass).to(args.device)\n",
    "        model.reset_parameters()\n",
    "\n",
    "        # Count params\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Run {run_idx+1}/{args.runs} | Params: {total_params}\")\n",
    "\n",
    "        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        best_val, best_state = 0.0, None\n",
    "\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            loss = train_one_epoch(model, train_loader, optimizer, args.device)\n",
    "            val_acc = evaluate(model, val_loader, args.device)\n",
    "            test_acc = evaluate(model, test_loader, args.device)\n",
    "\n",
    "            if epoch % args.echo_epoch == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:03d} | loss={loss:.4f} | val={val_acc:.3f} | test={test_acc:.3f}\")\n",
    "\n",
    "            if val_acc > best_val:\n",
    "                best_val = val_acc\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        # Evaluate best-by-val on test\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        test_acc = evaluate(model, test_loader, args.device)\n",
    "        val_hist.append(best_val)\n",
    "        test_hist.append(test_acc)\n",
    "        print(f\"Best val={best_val:.3f} | Final test={test_acc:.3f}\")\n",
    "\n",
    "    print(f\"\\n>>> {model_name}: Test Acc over {args.runs} runs: \"\n",
    "          f\"mean={np.mean(test_hist)*100:.2f}%  std={np.std(test_hist)*100:.2f}%\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurograph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
