{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
    "from nilearn.image import load_img\n",
    "from scipy.stats import zscore\n",
    "import torch\n",
    "from torch_geometric.data import Data,InMemoryDataset\n",
    "from random import randrange\n",
    "import math\n",
    "import zipfile\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import torch\n",
    "import NeuroGraph\n",
    "from NeuroGraph import preprocess\n",
    "import boto3\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "#from connectivity_matrices import KendallConnectivityMeasure\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "\n",
    "# This function is called by the parallel processing pool.\n",
    "# It unpacks the arguments and calls the actual processing method.\n",
    "def worker_function(args):\n",
    "    # Unpack the arguments that were prepared for each task\n",
    "    iid, behavioral_df, BUCKET_NAME, volume = args\n",
    "    \n",
    "    # Directly call the static processing method with the unpacked arguments\n",
    "    return Brain_Connectome_Rest_Download.get_data_obj_static(iid, behavioral_df, BUCKET_NAME, volume)\n",
    "\n",
    "\n",
    "class Brain_Connectome_Rest_Download(InMemoryDataset):\n",
    "    \n",
    "    def __init__(self, root,name,n_rois, threshold,path_to_data,n_jobs,s3, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.root, self.dataset_name,self.n_rois,self.graph_threshold,self.target_path,self.n_jobs,self.s3 = root, name,n_rois,threshold,path_to_data,n_jobs,s3\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.dataset_name+'.pt']\n",
    "    \n",
    "#input: atlas, fmri. output: roi * time series\n",
    "    @staticmethod\n",
    "    def extract_from_3d_no(volume, fmri):\n",
    "        ''' \n",
    "        Extract time-series data from a 3d atlas with non-overlapping ROIs.\n",
    "        \n",
    "        Inputs:\n",
    "            path_to_atlas = '/path/to/atlas.nii.gz'\n",
    "            path_to_fMRI = '/path/to/fmri.nii.gz'\n",
    "            \n",
    "        Output:\n",
    "            returns extracted time series # volumes x # ROIs\n",
    "        '''\n",
    "\n",
    "        subcor_ts = []\n",
    "        for i in np.unique(volume): #volume is the atlas\n",
    "            if i != 0: #create a mask for each roi\n",
    "    #             print(i)\n",
    "                bool_roi = np.zeros(volume.shape, dtype=int)\n",
    "                bool_roi[volume == i] = 1\n",
    "                bool_roi = bool_roi.astype(bool)\n",
    "    #             print(bool_roi.shape)\n",
    "                # extract time-series data for each roi\n",
    "                roi_ts_mean = []\n",
    "                for t in range(fmri.shape[-1]):#average fmri signal of each roi over time\n",
    "                    roi_ts_mean.append(np.mean(fmri[:, :, :, t][bool_roi]))\n",
    "                subcor_ts.append(np.array(roi_ts_mean))\n",
    "        Y = np.array(subcor_ts).T #Y=roi x time series\n",
    "        return Y\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_Adj_postive_perc(corr,graph_threshold):\n",
    "        corr_matrix_copy = corr.detach().clone()\n",
    "        threshold = np.percentile(corr_matrix_copy[corr_matrix_copy > 0],\n",
    "                                  100 - graph_threshold)\n",
    "        corr_matrix_copy[corr_matrix_copy < threshold] = 0\n",
    "        corr_matrix_copy[corr_matrix_copy >= threshold] = 1\n",
    "        return corr_matrix_copy\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_data_obj_static(iid,behavioral_data,BUCKET_NAME,volume):\n",
    "        try:\n",
    "\n",
    "            time_series_file_path = \"data/raw/HCPGender/time_series_400\"\n",
    "            print(\"check!\",iid)\n",
    "\n",
    "            time_series_file = os.path.join(time_series_file_path, f\"{iid}_time_series.npy\")\n",
    "\n",
    "            Ytm = np.load(time_series_file)\n",
    "            \n",
    "            zd_Ytm = (Ytm - np.nanmean(Ytm, axis=0)) / np.nanstd(Ytm, axis=0, ddof=1)\n",
    "            \n",
    "            #positive_threshold_value = 1\n",
    "            threshold = 30\n",
    "            positive_threshold_value = np.percentile(zd_Ytm[zd_Ytm > 0], 100 - threshold)\n",
    "            zd_Ytm[zd_Ytm < positive_threshold_value] = 0\n",
    "            #zd_Ytm[zd_Ytm >= positive_threshold_value] = 1\n",
    "            \n",
    "            conn = ConnectivityMeasure(kind='correlation')\n",
    "            zd_fc = conn.fit_transform([zd_Ytm])[0]\n",
    "            \n",
    "            np.fill_diagonal(zd_fc, 0)\n",
    "            corr = torch.tensor(zd_fc).to(torch.float)\n",
    "            \n",
    "            iid = int(iid)\n",
    "            gender = behavioral_data.loc[iid,'Gender']\n",
    "            g = 1 if gender==\"M\" else 0\n",
    "            labels = torch.tensor([g,behavioral_data.loc[iid,'AgeClass'],behavioral_data.loc[iid,'ListSort_AgeAdj'],behavioral_data.loc[iid,'PMAT24_A_CR']])\n",
    "            A = Brain_Connectome_Rest_Download.construct_Adj_postive_perc(corr, graph_threshold=5)\n",
    "            edge_index = A.nonzero().t().to(torch.long)\n",
    "            data = Data(x=corr, edge_index=edge_index, y=labels) \n",
    "        except:\n",
    "            return None\n",
    "        return data\n",
    "\n",
    "\n",
    "#         ...\n",
    "    def process(self):\n",
    "        behavioral_df = pd.read_csv(os.path.join(\"data/\",'HCP_behavioral.csv')).set_index('Subject')[['Gender','Age','ListSort_AgeAdj','PMAT24_A_CR']]\n",
    "        mapping = {'22-25':0, '26-30':1,'31-35':2,'36+':3}\n",
    "        behavioral_df['AgeClass'] = behavioral_df['Age'].replace(mapping)\n",
    "\n",
    "        dataset = []\n",
    "        BUCKET_NAME = 'hcp-openaccess'\n",
    "        \n",
    "        with open(os.path.join(\"data/\",\"ids.pkl\"),'rb') as f:\n",
    "            ids = pickle.load(f)\n",
    "\n",
    "        roi = fetch_atlas_schaefer_2018(n_rois=self.n_rois,yeo_networks=17, resolution_mm=2)\n",
    "        atlas = load_img(roi['maps'])\n",
    "        volume = atlas.get_fdata()\n",
    "        #data_list = Parallel(n_jobs=self.n_jobs)(delayed(self.get_data_obj_static)(iid,behavioral_df,BUCKET_NAME,volume) for iid in tqdm(test_ids))\n",
    "        tasks = [(iid, behavioral_df, BUCKET_NAME, volume) for iid in ids]\n",
    "        with Pool(self.n_jobs) as pool:\n",
    "            data_list = pool.map(worker_function, tasks)\n",
    "\n",
    "        dataset = [x for x in data_list if x is not None]\n",
    "        # print(len(dataset))\n",
    "        if self.pre_filter is not None:\n",
    "            dataset = [data for data in dataset if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            dataset = [self.pre_transform(data) for data in dataset]\n",
    "\n",
    "        data, slices = self.collate(dataset)\n",
    "        print(\"saving path:\",self.processed_paths[0])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        \n",
    "ACCESS_KEY = ''  # your connectomeDB credentials\n",
    "SECRET_KEY = ''\n",
    "s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Brain_Connectome_Rest_Download_lag(InMemoryDataset):\n",
    "    \n",
    "    def __init__(self, root,name,n_rois, threshold,path_to_data,n_jobs,s3,lag, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.root, self.dataset_name,self.n_rois,self.graph_threshold,self.target_path,self.n_jobs,self.s3,self.lag = root, name,n_rois,threshold,path_to_data,n_jobs,s3,lag\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.dataset_name+'.pt']\n",
    "    \n",
    "#input: atlas, fmri. output: roi * time series\n",
    "    @staticmethod\n",
    "    def extract_from_3d_no(volume, fmri):\n",
    "        ''' \n",
    "        Extract time-series data from a 3d atlas with non-overlapping ROIs.\n",
    "        \n",
    "        Inputs:\n",
    "            path_to_atlas = '/path/to/atlas.nii.gz'\n",
    "            path_to_fMRI = '/path/to/fmri.nii.gz'\n",
    "            \n",
    "        Output:\n",
    "            returns extracted time series # volumes x # ROIs\n",
    "        '''\n",
    "\n",
    "        subcor_ts = []\n",
    "        for i in np.unique(volume): #volume is the atlas\n",
    "            if i != 0: #create a mask for each roi\n",
    "    #             print(i)\n",
    "                bool_roi = np.zeros(volume.shape, dtype=int)\n",
    "                bool_roi[volume == i] = 1\n",
    "                bool_roi = bool_roi.astype(bool)\n",
    "    #             print(bool_roi.shape)\n",
    "                # extract time-series data for each roi\n",
    "                roi_ts_mean = []\n",
    "                for t in range(fmri.shape[-1]):#average fmri signal of each roi over time\n",
    "                    roi_ts_mean.append(np.mean(fmri[:, :, :, t][bool_roi]))\n",
    "                subcor_ts.append(np.array(roi_ts_mean))\n",
    "        Y = np.array(subcor_ts).T # Y=roi x time series\n",
    "        return Y\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_Adj_postive_perc(corr,graph_threshold):\n",
    "        corr_matrix_copy = corr.detach().clone()\n",
    "        threshold = np.percentile(corr_matrix_copy[corr_matrix_copy > 0],\n",
    "                                  100 - graph_threshold)\n",
    "        corr_matrix_copy[corr_matrix_copy < threshold] = 0\n",
    "        corr_matrix_copy[corr_matrix_copy >= threshold] = 1\n",
    "        return corr_matrix_copy\n",
    "    \n",
    "    @staticmethod\n",
    "    def expand_time_series(time_series, lag):\n",
    "        #time_series shape = (1200, 400) i.e. (timepoints, roi)\n",
    "        expanded_ts = []\n",
    "        num_time_points, num_rois = time_series.shape\n",
    "        #ts_length = num_time_points - lag\n",
    "        truncated_time_series = time_series[:-lag]\n",
    "        lagged_time_series = time_series[lag:]\n",
    "        expanded_ts.append(truncated_time_series)\n",
    "        print(\"lagged_time_series\", lagged_time_series.shape)\n",
    "        print(\"truncated_time_series\", truncated_time_series.shape)\n",
    "        expanded_ts.append(lagged_time_series)\n",
    "        return np.concatenate(expanded_ts, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_expanded_correlation_matrix(expanded_ts):\n",
    "        conn = ConnectivityMeasure(kind='correlation')\n",
    "        corr_matrix = conn.fit_transform([expanded_ts])[0]\n",
    "        np.fill_diagonal(corr_matrix, 0)\n",
    "        return corr_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_expanded_lagged_corr(time_series, lag):\n",
    "        #for i in range(num_lag):\n",
    "        expanded_ts = Brain_Connectome_Rest_Download.expand_time_series(time_series, lag)\n",
    "        expanded_corr_matrix = Brain_Connectome_Rest_Download.construct_expanded_correlation_matrix(expanded_ts)\n",
    "        print(\"expanded_corr_matrix\", expanded_corr_matrix.shape)\n",
    "\n",
    "        return expanded_corr_matrix\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_data_obj_static(iid,behavioral_data,BUCKET_NAME,volume,lag):\n",
    "        try:\n",
    "            time_series_file_path = \"data/raw/HCPGender/time_series_100\"\n",
    "            print(\"check!\",iid)\n",
    "            time_series_file = os.path.join(time_series_file_path, f\"{iid}_time_series.npy\")\n",
    "            \n",
    "            Ytm = np.load(time_series_file)\n",
    "            \n",
    "            zd_Ytm = (Ytm - np.nanmean(Ytm, axis=0)) / np.nanstd(Ytm, axis=0, ddof=1)\n",
    "            \n",
    "            threshold = 30\n",
    "            positive_threshold_value = np.percentile(zd_Ytm[zd_Ytm > 0], 100 - threshold)\n",
    "            zd_Ytm[zd_Ytm < positive_threshold_value] = 0\n",
    "            #zd_Ytm[zd_Ytm >= positive_threshold_value] = 1\n",
    "            \n",
    "            expanded_corr_matrix = Brain_Connectome_Rest_Download.construct_expanded_lagged_corr(zd_Ytm, lag)\n",
    "            \n",
    "            #lag_corr = expanded_corr_matrix[0:1000, 1000:2000]\n",
    "            #lag_corr = expanded_corr_matrix[0:400, 400:800]\n",
    "            lag_corr = expanded_corr_matrix[0:100, 100:200]\n",
    "            np.fill_diagonal(lag_corr, 0)\n",
    "            \n",
    "            #lag_corr_reverse = expanded_corr_matrix[1000:2000, 0:1000]\n",
    "            #lag_corr_reverse = expanded_corr_matrix[400:800, 0:400]\n",
    "            lag_corr_reverse = expanded_corr_matrix[100:200, 0:100]\n",
    "            np.fill_diagonal(lag_corr_reverse, 0)\n",
    "            \n",
    "            conn = ConnectivityMeasure(kind='correlation')\n",
    "            zd_fc = conn.fit_transform([zd_Ytm])[0]\n",
    "            np.fill_diagonal(zd_fc, 0)\n",
    "            corr_original = torch.tensor(zd_fc).to(torch.float)\n",
    "            A = Brain_Connectome_Rest_Download.construct_Adj_postive_perc(corr_original, graph_threshold=5)\n",
    "            edge_index = A.nonzero().t().to(torch.long)\n",
    "            \n",
    "            # Stack the matrices along a new dimension\n",
    "            concat_matrix = np.concatenate((zd_fc, lag_corr,lag_corr_reverse), axis=1)\n",
    "            corr = torch.tensor(concat_matrix).to(torch.float)\n",
    "            iid = int(iid)\n",
    "            gender = behavioral_data.loc[iid,'Gender']\n",
    "            g = 1 if gender==\"M\" else 0\n",
    "            labels = torch.tensor([g,behavioral_data.loc[iid,'AgeClass'],behavioral_data.loc[iid,'ListSort_AgeAdj'],behavioral_data.loc[iid,'PMAT24_A_CR']])\n",
    "            data = Data(x=corr, edge_index=edge_index, y=labels) \n",
    "        except:\n",
    "            return None\n",
    "        return data\n",
    "\n",
    "\n",
    "#         ...\n",
    "    def process(self):\n",
    "        path_doc = \"data/\"\n",
    "        behavioral_df = pd.read_csv(os.path.join(path_doc,'HCP_behavioral.csv')).set_index('Subject')[['Gender','Age','ListSort_AgeAdj','PMAT24_A_CR']]\n",
    "        mapping = {'22-25':0, '26-30':1,'31-35':2,'36+':3}\n",
    "        behavioral_df['AgeClass'] = behavioral_df['Age'].replace(mapping)\n",
    "\n",
    "        dataset = []\n",
    "        BUCKET_NAME = 'hcp-openaccess'\n",
    "        \n",
    "        with open(os.path.join(path_doc,\"ids.pkl\"),'rb') as f:\n",
    "            ids = pickle.load(f)\n",
    "        print(len(ids))\n",
    "        roi = fetch_atlas_schaefer_2018(n_rois=self.n_rois,yeo_networks=17, resolution_mm=2)\n",
    "        atlas = load_img(roi['maps'])\n",
    "        volume = atlas.get_fdata()\n",
    "        lag = self.lag\n",
    "        #data_list = Parallel(n_jobs=self.n_jobs)(delayed(self.get_data_obj_static)(iid,behavioral_df,BUCKET_NAME,volume) for iid in tqdm(test_ids))\n",
    "        tasks = [(iid, behavioral_df, BUCKET_NAME, volume,lag) for iid in ids]\n",
    "        with Pool(self.n_jobs) as pool:\n",
    "            data_list = pool.map(worker_function, tasks)\n",
    "\n",
    "        dataset = [x for x in data_list if x is not None]\n",
    "        # print(len(dataset))\n",
    "        if self.pre_filter is not None:\n",
    "            dataset = [data for data in dataset if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            dataset = [self.pre_transform(data) for data in dataset]\n",
    "\n",
    "        data, slices = self.collate(dataset)\n",
    "        print(\"saving path:\",self.processed_paths[0])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"data/rs_1000/rs_1000_pearson/\"\n",
    "name = \"HCPGender\"\n",
    "threshold = 20\n",
    "path_to_data = \"data/raw/HCPGender\"\n",
    "n_rois = 1000\n",
    "n_jobs = 20 \n",
    "rest_dataset_pearson = Brain_Connectome_Rest_Download(root,name,n_rois, threshold,path_to_data,n_jobs,s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1000, 1000], edge_index=[2, 47450], y=[4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_dataset_pearson[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"data/rs_1000/rs_1000_spearman/\"\n",
    "rest_dataset_spearman = Brain_Connectome_Rest_Download(root,name,n_rois, threshold,path_to_data,n_jobs,s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"data/rs_1000/rs_1000_kendall/\"\n",
    "rest_dataset_kendall = Brain_Connectome_Rest_Download(root,name,n_rois, threshold,path_to_data,n_jobs,s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Function to create edge union\n",
    "def create_edge_union(p_data, s_data, k_data):\n",
    "    # Get all unique edges from P, S, and K\n",
    "    all_edges = set(map(tuple, p_data.edge_index.t().tolist())) | \\\n",
    "                set(map(tuple, s_data.edge_index.t().tolist())) | \\\n",
    "                set(map(tuple, k_data.edge_index.t().tolist()))\n",
    "    return torch.tensor(list(all_edges), dtype=torch.long).t()\n",
    "\n",
    "# Function to process a single data set with edge union\n",
    "def process_single_data(p_data, s_data, k_data, l_data):\n",
    "    #lag_x = l_data.x[:, 1000:3000]\n",
    "    #lag_x = l_data.x[:, 400:1200]\n",
    "    lag_x = l_data.x[:, 100:300]\n",
    "    #node_features = torch.cat([p_data.x, s_data.x, k_data.x, lag_x], dim=1)\n",
    "    node_features = p_data.x\n",
    "    \n",
    "    # Create edge union\n",
    "    edge_index_union = create_edge_union(p_data, s_data, k_data)\n",
    "    \n",
    "    # Initialize edge features (num_edges * 3)\n",
    "    num_edges = edge_index_union.size(1)\n",
    "    edge_features = torch.zeros((num_edges, 3))  # Initialize with zeros\n",
    "    \n",
    "    # Create edge sets for P, S, and K\n",
    "    edge_set_p = set(map(tuple, p_data.edge_index.t().tolist()))\n",
    "    edge_set_s = set(map(tuple, s_data.edge_index.t().tolist()))\n",
    "    edge_set_k = set(map(tuple, k_data.edge_index.t().tolist()))\n",
    "    \n",
    "    # Fill in presence for edges in P, S, and K\n",
    "    for i, edge in enumerate(edge_index_union.t()):\n",
    "        edge_tuple = tuple(edge.tolist())\n",
    "        if edge_tuple in edge_set_p:\n",
    "            edge_features[i, 0] = 1  # First column for Pearson\n",
    "        if edge_tuple in edge_set_s:\n",
    "            edge_features[i, 1] = 1  # Second column for S\n",
    "        if edge_tuple in edge_set_k:\n",
    "            edge_features[i, 2] = 1  # Third column for K\n",
    "\n",
    "    # Filter out edges not in Pearson (if needed)\n",
    "    pearson_mask = edge_features[:, 0] == 1  # Filter by Pearson edges\n",
    "    filtered_edge_index = edge_index_union[:, pearson_mask]\n",
    "    filtered_edge_features = edge_features[pearson_mask]\n",
    "\n",
    "    # Create new Data object with filtered edges\n",
    "    new_data = Data(x=node_features, edge_index=filtered_edge_index, edge_attr=filtered_edge_features, y=p_data.y)\n",
    "    return new_data\n",
    "\n",
    "# Parallel processing for combined dataset\n",
    "def create_combined_dataset_parallel(P, S, K, L, num_workers=20):\n",
    "    with Pool(num_workers) as pool:\n",
    "        results = pool.starmap(process_single_data, zip(P, S, K, L))\n",
    "    return results\n",
    "\n",
    "class PSKLL_PSK_Dataset(InMemoryDataset):\n",
    "\n",
    "    def __init__(self, root, dataset_name, dataset, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.root, self.dataset_name, self.dataset = root, dataset_name, dataset\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        #return [self.dataset_name + '_PSKLL_PSK.pt']\n",
    "        return [self.dataset_name + '_PSK.pt']\n",
    "\n",
    "    def process(self):\n",
    "        gender_dataset = []\n",
    "        for d in self.dataset:\n",
    "            data = Data(x=d.x, edge_index=d.edge_index, edge_attr=d.edge_attr, y=d.y)\n",
    "            gender_dataset.append(data)\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            gender_dataset = [data for data in gender_dataset if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            gender_dataset = [self.pre_transform(data) for data in gender_dataset]\n",
    "\n",
    "        data, slices = self.collate(gender_dataset)\n",
    "        print(\"saving path:\", self.processed_paths[0])\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving path: data/rs_1000/rs_1000_PSK/processed/HCPGender_PSK.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1000, 1000], edge_index=[2, 47450], edge_attr=[47450, 3], y=[4])\n"
     ]
    }
   ],
   "source": [
    "lag = 1\n",
    "root = \"data/rs_100/rs_100_thre30_\"+str(lag)+\"lag/\"\n",
    "rest_dataset_lag = Brain_Connectome_Rest_Download_lag(root,name,n_rois, threshold,path_to_data,n_jobs,s3,lag)\n",
    "new_dataset = create_combined_dataset_parallel(rest_dataset_pearson, rest_dataset_spearman, rest_dataset_kendall, rest_dataset_lag)\n",
    "#root = 'data/rs_100/rs_100_thre30_'+str(lag)+'lag_PSKLL_PSK/'\n",
    "root = 'data/rs_1000/rs_1000_PSK/'\n",
    "dataset = PSKLL_PSK_Dataset(root, name, new_dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data,InMemoryDataset\n",
    "import logging\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "\n",
    "class PSKLL_PSK_Dataset(InMemoryDataset):\n",
    "\n",
    "    def __init__(self, root,dataset_name, dataset,transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.root, self.dataset_name, self.dataset = root, dataset_name,dataset\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.dataset_name+'_PSK.pt']\n",
    "\n",
    "    def process(self):\n",
    "        gender_dataset = []\n",
    "        for d in self.dataset:\n",
    "            data = Data(x= d.x, edge_index=d.edge_index, edge_attr=d.edge_attr, y=d.y)\n",
    "            gender_dataset.append(data)\n",
    "        if self.pre_filter is not None:\n",
    "            gender_dataset = [data for data in gender_dataset if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            gender_dataset = [self.pre_transform(data) for data in gender_dataset]\n",
    "\n",
    "        data, slices = self.collate(gender_dataset)\n",
    "        print(\"saving path:\",self.processed_paths[0])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        \n",
    "class PSKLL_PSK_Gender_Dataset(InMemoryDataset):\n",
    "\n",
    "    def __init__(self, root,dataset_name, dataset,transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.root, self.dataset_name, self.dataset = root, dataset_name,dataset\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.dataset_name+'_PSK_Gender.pt']\n",
    "\n",
    "    def process(self):\n",
    "        gender_dataset = []\n",
    "        for d in self.dataset:\n",
    "            labels = d.y\n",
    "            gender = labels[0].item()\n",
    "            data = Data(x= d.x, edge_index=d.edge_index, edge_attr=d.edge_attr, y = int(gender))\n",
    "            gender_dataset.append(data)\n",
    "        if self.pre_filter is not None:\n",
    "            gender_dataset = [data for data in gender_dataset if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            gender_dataset = [self.pre_transform(data) for data in gender_dataset]\n",
    "\n",
    "        data, slices = self.collate(gender_dataset)\n",
    "        print(\"saving path:\",self.processed_paths[0])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch import nn\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch_geometric.nn import aggr\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import APPNP, MLP, GCNConv, GINConv, SAGEConv, GraphConv, TransformerConv, ChebConv, GATConv, SGConv, GeneralConv ,RGCNConv\n",
    "from torch.nn import Conv1d, MaxPool1d, ModuleList\n",
    "import math\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "from itertools import product\n",
    "softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "class Args:\n",
    "    dataset = 'HCPGender'\n",
    "    runs = 3\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    seed = 123\n",
    "    model_list = [\"GCNConv\"]\n",
    "    hidden = 32\n",
    "    hidden_mlp = 64\n",
    "    num_layers = 3\n",
    "    epochs = 100\n",
    "    echo_epoch = 50\n",
    "    batch_size = 16\n",
    "    early_stopping = 50\n",
    "    lr = 5e-4\n",
    "    weight_decay = 0.0005\n",
    "    dropout = 0.5\n",
    "    edge_feature_dim = 3\n",
    "args = Args()\n",
    "\n",
    "\n",
    "path = \"base_params_test_rs_edge_feature/\"\n",
    "res_path = \"results/\"\n",
    "root = \"data/\"\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "if not os.path.isdir(res_path):\n",
    "    os.mkdir(res_path)\n",
    "def logger(info):\n",
    "    f = open(os.path.join(res_path, 'results_new.csv'), 'a')\n",
    "    print(info, file=f)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear, Conv1d, MaxPool1d, ModuleList\n",
    "\n",
    "\n",
    "class GCNConvWithEdgeAttr(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_feature_dim):\n",
    "        super(GCNConvWithEdgeAttr, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.edge_lin = torch.nn.Linear(edge_feature_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Normalize edge weights.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Propagate messages.\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr, norm=norm)\n",
    "\n",
    "    def message(self, x_j, edge_attr, norm):\n",
    "        edge_attr = self.edge_lin(edge_attr)\n",
    "        return norm.view(-1, 1) * (x_j + edge_attr)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "class GNNsWithEdgeAttr(torch.nn.Module):\n",
    "    def __init__(self, args, train_dataset, hidden_channels, num_layers, GNNWithEdgeAttr, edge_feature_dim, k=0.6):\n",
    "        super().__init__()\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in train_dataset])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = max(10, k)\n",
    "        self.k = int(k)\n",
    "        self.sort_aggr = aggr.SortAggregation(self.k)\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GNNWithEdgeAttr(train_dataset.num_features, hidden_channels, edge_feature_dim))\n",
    "        for i in range(0, num_layers - 1):\n",
    "            self.convs.append(GNNWithEdgeAttr(hidden_channels, hidden_channels, edge_feature_dim))\n",
    "        self.convs.append(GNNWithEdgeAttr(hidden_channels, 1, edge_feature_dim))\n",
    "        \n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_channels * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0], conv1d_kws[0])\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1], conv1d_kws[1], 1)\n",
    "        dense_dim = int((self.k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.mlp = MLP([dense_dim, 32, args.num_classes], dropout=0.5, batch_norm=False)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.mlp.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [conv(xs[-1], edge_index, edge_attr).tanh()]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        x = self.sort_aggr(x, batch)\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.conv2(x).relu()\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class GNNsWithEdgeAttrGAT(torch.nn.Module):\n",
    "    def __init__(self, args, train_dataset, hidden_channels, num_layers, edge_feature_dim, heads=1, k=0.6):\n",
    "        super().__init__()\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in train_dataset])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = max(10, k)\n",
    "        self.k = int(k)\n",
    "        self.sort_aggr = aggr.SortAggregation(self.k)\n",
    "        \n",
    "        # Define the GAT convolution layers\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GATConv(train_dataset.num_features, hidden_channels, heads=heads, concat=False, edge_dim=edge_feature_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GATConv(hidden_channels, hidden_channels, heads=heads, concat=False, edge_dim=edge_feature_dim))\n",
    "        self.convs.append(GATConv(hidden_channels, 1, heads=1, concat=False, edge_dim=edge_feature_dim))\n",
    "\n",
    "        # Define the 1D convolution and MLP layers\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_channels * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0], conv1d_kws[0])\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1], conv1d_kws[1], 1)\n",
    "        dense_dim = int((self.k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.mlp = MLP([dense_dim, 32, args.num_classes], dropout=0.5, batch_norm=False)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.mlp.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [conv(xs[-1], edge_index, edge_attr=edge_attr).tanh()]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        x = self.sort_aggr(x, batch)\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.conv2(x).relu()\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class SAGEConvWithEdgeAttr(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_feature_dim):\n",
    "        super(SAGEConvWithEdgeAttr, self).__init__(aggr='mean')\n",
    "        self.node_lin = nn.Linear(in_channels, out_channels)\n",
    "        self.edge_lin = nn.Linear(edge_feature_dim, out_channels)\n",
    "        self.agg_lin = nn.Linear(out_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.node_lin(x)\n",
    "        \n",
    "        edge_index, edge_attr = add_self_loops(edge_index, edge_attr=edge_attr, num_nodes=x.size(0))\n",
    "\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        edge_attr = self.edge_lin(edge_attr)\n",
    "        \n",
    "        return x_j + edge_attr\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        return scatter(inputs, index, dim=0, reduce='mean')\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        \n",
    "        aggr_out = torch.cat([x, aggr_out], dim=-1)\n",
    "        \n",
    "        \n",
    "        return self.agg_lin(aggr_out)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(args.device)\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)\n",
    "        total_loss +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return total_loss/len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(args.device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "name = \"HCPGender\"\n",
    "threshold = 5\n",
    "n_rois = 100\n",
    "n_jobs = 15 # this script runs in parallel and requires the number of jobs is an input\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the hyperparameter grid for grid search\n",
    "log_filename = f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "\n",
    "for roi in [100,400,1000]:\n",
    "        param_grid = {\n",
    "            'model': ['GCNConv', 'GATConv', 'SAGEConv'],  # Model types to search\n",
    "            'hidden': [32],               # Hidden layer sizes\n",
    "            #'hidden_mlp': [32, 64, 128],         # Hidden layer sizes for MLP\n",
    "            #'batch_size': [16, 32, 64],          # Batch sizes to try\n",
    "            'num_layers': [3],          # Number of layers\n",
    "            'lr': [5e-4],         # Learning rates to try\n",
    "            'dropout': [0.5],            # Dropout rates\n",
    "            'weight_decay': [0.0005],     # Weight decay values\n",
    "        }\n",
    "        \n",
    "        root = 'data/rs_'+str(roi)+'/rs_'+str(roi)+'_PSK/'\n",
    "        rs_dataset = PSKLL_PSK_Dataset(root, name, None)\n",
    "        dataset = PSKLL_PSK_Gender_Dataset(root, name, rs_dataset)\n",
    "        labels = [d.y.item() for d in dataset]\n",
    "        train_tmp, test_indices = train_test_split(list(range(len(labels))),\n",
    "                                test_size=0.2, stratify=labels,random_state=123,shuffle= True)\n",
    "        tmp = dataset[train_tmp]\n",
    "        train_labels = [d.y.item() for d in tmp]\n",
    "        train_indices, val_indices = train_test_split(list(range(len(train_labels))),\n",
    "        test_size=0.125, stratify=train_labels,random_state=123,shuffle = True)\n",
    "        train_dataset = tmp[train_indices]\n",
    "        val_dataset = tmp[val_indices]\n",
    "        test_dataset = dataset[test_indices]\n",
    "        #print(\"dataset {} loaded with train {} val {} test {} splits\".format(args.dataset,len(train_dataset), len(val_dataset), len(test_dataset)))\n",
    "        logging.info(\"Dataset %s loaded with train %d val %d test %d splits\", args.dataset, len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, args.batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, args.batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, args.batch_size, shuffle=False)\n",
    "        args.num_features,args.num_classes = dataset.num_features,dataset.num_classes\n",
    "\n",
    "        \n",
    "        for params in product(*param_grid.values()):\n",
    "            args.model, args.hidden, args.num_layers, args.lr, args.dropout, args.weight_decay = params\n",
    "            \n",
    "            val_acc_history, test_acc_history, test_loss_history = [],[],[]\n",
    "            seeds = [123,124,125,126,127,128,129,221,223,224,228,229]\n",
    "            \n",
    "                \n",
    "            for index in range(args.runs):\n",
    "                start = time.time()\n",
    "                torch.manual_seed(seeds[index])\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.manual_seed(seeds[index])\n",
    "                random.seed(seeds[index])\n",
    "                np.random.seed(seeds[index])\n",
    "                torch.backends.cudnn.deterministic = True\n",
    "                torch.backends.cudnn.benchmark = False\n",
    "\n",
    "                #gnn = eval(args.model)            \n",
    "                # Initialize model based on the current hyperparameter set\n",
    "                if args.model == \"GCNConv\":\n",
    "                    model = GNNsWithEdgeAttr(\n",
    "                        args, train_dataset, hidden_channels=args.hidden,\n",
    "                        num_layers=args.num_layers, GNNWithEdgeAttr=GCNConvWithEdgeAttr,\n",
    "                        edge_feature_dim=args.edge_feature_dim\n",
    "                    ).to(args.device)\n",
    "                elif args.model == \"SAGEConv\":\n",
    "                    model = GNNsWithEdgeAttr(\n",
    "                        args, train_dataset, hidden_channels=args.hidden,\n",
    "                        num_layers=args.num_layers, GNNWithEdgeAttr=SAGEConvWithEdgeAttr,\n",
    "                        edge_feature_dim=args.edge_feature_dim\n",
    "                    ).to(args.device)\n",
    "                elif args.model == \"GATConv\":\n",
    "                    model = GNNsWithEdgeAttrGAT(\n",
    "                        args, train_dataset, hidden_channels=args.hidden,\n",
    "                        num_layers=args.num_layers, edge_feature_dim=args.edge_feature_dim,\n",
    "                        heads=4\n",
    "                    ).to(args.device)\n",
    "                \n",
    "                #print(model)\n",
    "                logging.info(model)\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                #print(f\"Total number of parameters is: {total_params}\")\n",
    "                logging.info(\"Total number of parameters is: %d\", total_params)\n",
    "                \n",
    "                optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "                loss, test_acc = [],[]\n",
    "                best_val_acc,best_val_loss = 0.0,0.0\n",
    "                for epoch in range(args.epochs):\n",
    "                    loss = train(train_loader)\n",
    "                    val_acc = test(val_loader)\n",
    "                    test_acc = test(test_loader)\n",
    "                    if epoch%10==0:\n",
    "                        logging.info(\"Epoch: %d, Loss: %.6f, Val Acc: %.2f, Test Acc: %.2f\", epoch, loss.item(), val_acc, test_acc)\n",
    "                        print(\"epoch: {}, loss: {}, val_acc:{}, test_acc:{}\".format(epoch, np.round(loss.item(),6), np.round(val_acc,2),np.round(test_acc,2)))\n",
    "                    val_acc_history.append(val_acc)\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        \n",
    "                        print(\"best val acc:\",best_val_acc)\n",
    "                        logging.info(\"Best Val Acc: %.2f\", best_val_acc)\n",
    "                        torch.save(model.state_dict(), path + args.dataset+args.model+'task-checkpoint-best-acc.pkl')\n",
    "                model.load_state_dict(torch.load(path + args.dataset+args.model+'task-checkpoint-best-acc.pkl'))\n",
    "                model.eval()\n",
    "                test_acc = test(test_loader)\n",
    "                test_loss = train(test_loader).item()\n",
    "                test_acc_history.append(test_acc)\n",
    "                test_loss_history.append(test_loss)\n",
    "            \n",
    "            end = time.time()\n",
    "\n",
    "            log_msg = \"lag: {}, model:{}, Hidden: {}, layers: {}, lr: {}, dropout:{}, weightdecay:{}, Loss: {:.4f}, Acc: {:.2f}, Std: {:.2f}, Running Time: {:.2f}\".format(\n",
    "            lag, args.model, args.hidden, args.num_layers, args.lr, args.dropout, args.weight_decay, \n",
    "            np.mean(test_loss_history), np.mean(test_acc_history) * 100, np.std(test_acc_history) * 100, end - start\n",
    "            )\n",
    "            logging.info(log_msg)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurograph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
